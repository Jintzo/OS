\section{Inter Process Communication}

\paragraph{Overview}
\begin{itemize}
  \item \textbf{Reasons} for cooperating processes:
  \begin{itemize}
    \item \emph{information sharing}: share file/data-structure in memory
    \item \emph{computation speed-up}: break large tasks in subtasks \( \leadsto \) parallel execution
    \item \emph{modularity}: divide system into collaborating modules with clean interfaces
  \end{itemize}
  \item \textbf{IPC}: allows data exchange
  \begin{itemize}
    \item \emph{message passing}: explicitly send/receive information using syscalls
    \item \emph{shared memory}: physical memory region used by multiple processes/threads
  \end{itemize}
\end{itemize}

\paragraph{IPC --- message passing}
\begin{itemize}
  \item = mechanism for processes to communicate and synchronize
  \item message passing facilities generally provide \code{send} and \code{receive}
  \item \textbf{Implementations}:
  \begin{itemize}
    \item hardware bus
    \item shared memory
    \item kernel memory
    \item network interface card (NIC)
  \end{itemize}
  \item \textbf{Direct messages}: processes explicitly named when exchanging messages
  \item \textbf{Indirect messages}: sending to/receiving from \emph{mailboxes}
  \begin{itemize}
    \item first communicating process creates mailbox, last destroys
    \item processes can only communicate through shared mailbox
  \end{itemize}
\end{itemize}

\paragraph{Indirect messages -- synchronization}
\begin{itemize}
  \item \textbf{Blocking} (synchronous):
  \begin{itemize}
    \item \emph{blocking send}: sender blocks until message is received
    \item \emph{blocking receive}: receiver blocks until message is available
  \end{itemize}
  \item \textbf{Non-blocking} (asynchronous):
  \begin{itemize}
    \item \emph{non-blocking send}: sender sends message, then continues
    \item \emph{non-blocking receive}: receiver receives valid message or \code{null}
  \end{itemize}
\end{itemize}

\paragraph{Messaging --- Buffering}
\begin{itemize}
  \item messages are \emph{queued} using different capacities while being in-flight
  \item \textbf{zero capacity}: no queuing
  \begin{itemize}
    \item \emph{rendezvous}: sender must wait for receiver
    \item message is transferred as soon as receiver becomes available \( \to \) no latency/jitter
  \end{itemize}
  \item \textbf{bounded capacity}: finite number + length of messages
  \begin{itemize}
    \item sender can send before receiver waits for messages
    \item sender must wait if link is full
  \end{itemize}
  \item \textbf{unbounded capacity}:
  \begin{itemize}
    \item sender never waits
    \item memory may overflow \( \to \) potentially large latency/jitter between \code{send} and \code{receive}
  \end{itemize}
\end{itemize}

\paragraph{Messaging --- POSIX message queues}
\begin{itemize}
  \item \textbf{create} or open existing message queue: \\*
    \code{mqd_t mq_open (const char *name, int oflag);}
    \begin{itemize}
      \item \code{name} ist path in file system
      \item access permission controlled through file system access permission
    \end{itemize}
  \item \textbf{send} message to message queue: \\*
    \code{int mq_send (mqd_t md, const char *msg, size_t len, unsigned priority);}
  \item \textbf{receive} message with highest priority in message queue: \\*
    \code{int mq_receive (mqd_t md, char *msg, size_t len, unsigned *priority);}
  \item \textbf{register} callback handler on message queue (to avoid polling): \\*
    \code{int mq_notify (mqd_t md, const struct sigevent *sevp);}
  \item \textbf{remove} message queue: \\*
    \code{int mq_unlink (const char *name);}
\end{itemize}

\paragraph{Shared Memory}
\begin{itemize}
  \item \textbf{Principle}: communicate through region of shared memory
  \begin{itemize}
    \item every write to shared region is visible to all other processes
    \item hardware guarantees that always most recent write is read
  \end{itemize}
  \item \textbf{Implementation}: message passing via shared memory is application-specific
  \item \textbf{Problems}: using shared memory in a safe way is tricky
  \begin{itemize}
    \item \emph{cache coherency protocol}: makes usage with many processes/CPUs hard
    \item \emph{race conditions}: makes usage with multiple writers hard
  \end{itemize}
\end{itemize}

\paragraph{Shared Memory --- POSIX shared memory}
\begin{itemize}
  \item \textbf{create} or open existing POSIX shared memory object: \\*
    \code{int shm_open (const char *name, int oflag, mod_t mode);}
  \item \textbf{set} size of shared memory region: \\*
    \code{ftruncate (smd, size_t len);}
  \item \textbf{map} shared memory object to address space: \\*
    \code{void* mmap (void* addr, size_t len, [...], smd, [...]);}
  \item \textbf{unmap} shared memory object from address space: \\*
    \code{int munmap (void* addr, size_t len);}
  \item \textbf{destroy} shared memory object: \\*
    \code{int shm_unlink (const char *name);}
\end{itemize}

\paragraph{Shared Memory --- sequential memory consistency}
\begin{itemize}
  \item[=] \emph{the result of execution as if all operations were executed in some sequential order, and the operations of each processor occurred in the order specified by the program.}
  \item \textbf{Model}:
  \begin{itemize}
    \item all memory operations occur one at a time in \emph{program order}
    \item ensures write atomicity
  \end{itemize}
  \item \textbf{Reality}: compiler and CPU re-order instructions to \emph{execution order}
  \begin{itemize}
    \item[$ \to $] without SC many processes on many CPU behave worse than preemptive threads on 1 CPU
  \end{itemize}
\end{itemize}

\paragraph{Shared Memory --- memory consistency model}
\begin{itemize}
  \item \textbf{Problem}:
  \begin{itemize}
    \item CPUs generally not sequentially consistent
    \item compilers do not generate code in program order
  \end{itemize}
\end{itemize}

\paragraph{Synchronization --- race conditions}
\begin{itemize}
  \item \textbf{Assume}: sequential memory consistency \( \to \) no atomic memory transactions!
  \item \textbf{Critical Sections}: protect instructions inside critical section from concurrent execution
\end{itemize}

\paragraph{Critical Sections --- desired properties}
\begin{itemize}
  \item \textbf{mutual exclusion}: at most one thread can be in the CS at any time
  \item \textbf{progress}: no thread running outside of CS may block other thread from getting in
  \item \textbf{bounded waiting}: once a thread starts trying to enter CS, there is a bound on number of times other threads get in
\end{itemize}

\paragraph{Critical Sections --- disabling interrupts}
\begin{itemize}
  \item kernel only switches on interrupts (usually on \emph{timer interrupt})
  \begin{itemize}
    \item[$ \to $] have per-thread \emph{do not interrupt} (DNI)-bit
  \end{itemize}
  \item \textbf{single-core system}:
  \begin{itemize}
    \item enter CS: set DNI bit
    \item leave CS: clear DNI bit
  \end{itemize}
  \item \textbf{Advantages}:
  \begin{itemize}
    \item[+] easy + convenient in kernel
  \end{itemize}
  \item \textbf{Disadvantages}:
  \begin{itemize}
    \item[-] \emph{only works on single-core systems}: disabling interrupts on one CPU doesn't affect other CPUs
    \item[-] \emph{only feasible in kernel}: don't want to give user power to turn off interrupts!
  \end{itemize}
\end{itemize}

\paragraph{Critical Sections --- lock variables}
\begin{itemize}
  \item define global \code{lock} variable
  \begin{itemize}
    \item only enter CS if \code{lock} is \code{0}, set to \code{1} on enter
    \item wait for lock to become \code{0} otherwise (\emph{busy waiting})
  \end{itemize}
  \item \textbf{Problem}: doesn't solve CS problem! Reading/Setting lock not atomic!
\end{itemize}

\paragraph{Critical Sections --- spinlocks}
\begin{itemize}
  \item to make lock variable approach work, lock variable must be tested and set at same time atomically:
  \item \textbf{x86}: \code{xchg} can atomically exchange memory content with register
  \begin{itemize}
    \item exchanges register content with memory content
    \item returns previous memory content of lock
    \item[$ \leadsto $] implementation of critical section as \emph{spinlock}:
  \end{itemize}
  \begin{lstlisting}[language=c]
void enter_critical_section (volatile bool *lock) {
  while (xchg(lock, 1) == 1); // lock = 1, return old value
                                 // repeat until old value != 1
}

void leave_critical_section (volatile bool *lock) {
  *lock = 0;
}
  \end{lstlisting}
  \item \textbf{Advantages}:
  \begin{itemize}
    \item[+] \emph{mutual exclusion}: only one thread can enter CS
    \item[+] \emph{progress}: only thread within CS hinders others of getting in
  \end{itemize}
  \item \textbf{Disadvantages}:
  \begin{itemize}
    \item[-] \emph{bounded waiting}: no upper bound
  \end{itemize}
\end{itemize}

\paragraph{Spinlocks --- Limitations}
\begin{itemize}
  \item \textbf{Congestion}:
  \begin{itemize}
    \item if most times there is no thread in CS when another tries to enter, then spinlocks are very easy + efficient
    \item if CS is large or many threads try to enter, spinlocks might not be good choice as all threads actively wait spinning
  \end{itemize}
  \item \textbf{Multicore}: memory address is written at every atomic swap operation
  \begin{itemize}
    \item[$ \to $] memory is expensively kept coherent
  \end{itemize}
  \item \textbf{Static Priorities} (e.g., \emph{priority inversion}): if low-priority threads hold lock it will never be able to release it, because it will never be scheduled
\end{itemize}

\paragraph{Spinlocks --- sleep while wait}
\begin{itemize}
  \item \textbf{Problem}: busy part of busy waiting
  \begin{itemize}
    \item wastes resources,
    \item stresses cache coherence protocol,
    \item can cause priority inversion problem
  \end{itemize}
  \item \textbf{Idea}:
  \begin{itemize}
    \item threads sleep on locks if occupied
    \item wake up threads one at a time when lock becomes free
  \end{itemize}
\end{itemize}

\paragraph{Spinlocks --- semaphore}
\begin{itemize}
  \item two new syscalls operating on \code{int} variables:
  \begin{itemize}
    \item \code{wait (&s)}: if \code{s > 0}: \code{s--} and continue, otherwise let caller sleep
    \item \code{signal (&s)}: if no thread is waiting: \code{s++}, otherwise wake one up
  \end{itemize}
  \item initialize \code{s} to maximum number of threads that may enter CS
  \begin{itemize}
    \item \code{wait} = \code{enter_critical_section()}
    \item \code{signal} = \code{leave_critical_section()}
  \end{itemize}
  \item \textbf{mutex} (semaphore): semaphore initialized to 1 (only admits one thread at a time into CS)
  \item \textbf{counting semaphore}: semaphore allowing more than one thread into CS at a time
\end{itemize}

\paragraph{Semaphore --- implementation}
\begin{itemize}
  \item \code{wait} and \code{signal} calls need to be carefully synchronized (otherwise \emph{race condition} between checking and decrementing \code{s})
  \item \textbf{signal loss} can occur when waiting and waking threads up at same time
  \item[$ \leadsto $] each semaphore has \textbf{wake-up queue}:
  \begin{itemize}
    \item \emph{weak semaphores}: wake up random waiting thread on \code{signal}
    \item \emph{strong semaphores}: wake up thread strictly in order which they started \code{wait}ing
  \end{itemize}
  \item \textbf{Advantages}:
  \begin{itemize}
    \item[+] \emph{mutual exclusion}: only one thread can enter CS for mutexes
    \item[+] \emph{progress}: only thread within CS hinders others to get in
    \item[+] \emph{bounded waiting}: strong semaphores guarantee bounded waiting
  \end{itemize}
  \item \textbf{Disadvantages}:
  \begin{itemize}
    \item[-] every enter and exit of CS is syscall \( \to \) slow
  \end{itemize}
\end{itemize}

\paragraph{Fast User Space mutex}
\begin{itemize}
  \item \textbf{spinlock}:
  \begin{itemize}
    \item[+] quick when wait-time is short
    \item[-] waste resources when wait-time is long
  \end{itemize}
  \item \textbf{semaphore}:
  \begin{itemize}
    \item[+] efficient when wait-time is long
    \item[-] syscall overhead at every operation
  \end{itemize}
  \item \textbf{futex}:
  \begin{itemize}
    \item userspace + kernel component
    \item try to get into CS with userspace spinlock
    \begin{itemize}
      \item CS busy \( \to \) use syscall to put thread to sleep
      \item otherwise \( \to \) enter CS with now locked spinlock completely in userspace
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{summary}
  \begin{itemize}
    \item \textbf{communication} between processes/threads often needed
    \begin{itemize}
      \item \emph{message passing}: provide explicit send/receive functions to exchange messages
      \item \emph{implicitly/explicitly shared memory} between threads/processes: allows information exchange
    \end{itemize}
    \item \textbf{data races}: need to be taken into account when communicating
    \item \textbf{synchronization techniques}:
    \begin{itemize}
      \item interlocked atomic operations
      \item spinlocks
      \item semaphores
      \item futexes
    \end{itemize}
  \end{itemize}
\end{summary}